from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import random
import pandas as pd
import os

#–ù–∞—Å—Ç—Ä–æ–π–∫–∏
SEARCH_TOPICS = [
    "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
    "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤",
    "–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞",
    "–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –æ—Å–Ω–æ–≤—ã CS",
    "–ö—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "–†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è",
    "–ß–µ–ª–æ–≤–µ–∫–æ-–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ",
    "–ö–æ–º–ø—å—é—Ç–µ—Ä—ã –∏ –æ–±—â–µ—Å—Ç–≤–æ",
    "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫",
    "–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞"
]

NUM_PAGES = 15
base_url = "https://ya.ru/search/ "
lr = "213"  #—Ä–µ–≥–∏–æ–Ω: –ú–æ—Å–∫–≤–∞
OUTPUT_FILE = "all_links.csv"

#–°–ø–∏—Å–æ–∫ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
BAD_DOMAINS = {
    #–í–∏–¥–µ–æ
    'youtube.com', 'youtu.be', 'rutube.ru', 'vimeo.com', 'dailymotion.com',
    #–°–æ—Ü—Å–µ—Ç–∏
    'vk.com', 'telegram.org', 't.me', 'instagram.com', 'facebook.com',
    'twitter.com', 'x.com', 'ok.ru', 'pinterest.com',
    #–ú–∞–≥–∞–∑–∏–Ω—ã
    'aliexpress.', 'amazon.', 'wildberries', 'ozon.', 'my-shop.', 'labirint.',
    'chitai-gorod.', 'gearbest.', 'market.yandex.', 'shop.', 'store.',
    #–§–æ—Ä—É–º—ã / –ë–ª–æ–≥–∏
    'wordpress.'
    #–†–µ–∫–ª–∞–º–∞ / —Ç—Ä–µ–∫–µ—Ä—ã
    'taboola', 'yabs.', 'clck.', 'ad.mail.ru', 'googleadservices.',
    #–ù–æ–≤–æ—Å—Ç–∏ / –º–µ–¥–∏–∞
    'news', 'gazeta.', 'lenta.', 'kommersant.', 'rbc.', 'forbes.', '161.ru',
    #–ü—Ä–æ—á–µ–µ
    'telegram.', 'gov.ru', 'student.zoomru.ru', 'nikulya.'
}

#–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫
if not os.path.exists("collected_links"):
    os.makedirs("collected_links")

#–ù–∞—Å—Ç—Ä–æ–π–∫–∏ Selenium
def setup_browser():
    PATH = r'C:\Study\Python\WebsiteClassification\bot\chromedriver.exe'
    options = Options()
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36")
    
    #–ê–Ω—Ç–∏–±–æ—Ç-–∑–∞—â–∏—Ç–∞
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)

    service = Service(executable_path=PATH)
    driver = webdriver.Chrome(service=service, options=options)

    #–°–∫—Ä—ã–≤–∞–µ–º —Å–ª–µ–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

#–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ–π –ø–∞—É–∑—ã
def random_pause(min_sec=5, max_sec=8):
    pause = random.uniform(min_sec, max_sec)
    print(f"–ü–∞—É–∑–∞ {pause:.2f} —Å–µ–∫—É–Ω–¥...")
    time.sleep(pause)

#–û–∂–∏–¥–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è CAPTCHA –≤—Ä—É—á–Ω—É—é
def wait_for_captcha(driver):
    try:
        print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ CAPTCHA...")
        captcha_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[contains(text(), "–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç")]'))
        )
        if captcha_element:
            print("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CAPTCHA. –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç.")
            input("–ù–∞–∂–º–∏—Ç–µ Enter –ø–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è CAPTCHA...")

            #–ñ–¥—ë–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            WebDriverWait(driver, 60).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'li.serp-item_card[data-fast="1"]'))
            )
            print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –ø–æ—Å–ª–µ CAPTCHA")
            return True
    except Exception as e:
        print("CAPTCHA –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        return False

#–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤—ã–¥–∞—á–∏
def parse_yandex_results(html, topic):
    soup = BeautifulSoup(html, 'html.parser')
    links = []

    items = soup.select('li.serp-item_card[data-fast="1"]')
    for item in items:
        try:
            link_tag = item.select_one('a.link')
            if link_tag and 'href' in link_tag.attrs:
                link = link_tag['href']

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ BAD_DOMAIN
                if any(domain in link for domain in BAD_DOMAINS):
                    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Å—ã–ª–∫–∞: {link} (–Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –¥–æ–º–µ–Ω)")
                    continue

                if link.startswith("https://"):
                    links.append({"url": link, "topic": topic})
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            continue

    return links

#–°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –ø–æ –æ–¥–Ω–æ–π –∑–∞–ø—Ä–æ—Å—É
def collect_links(query, num_pages=5, lr="213"):
    driver = setup_browser()
    collected_data = []

    try:
        for page in range(num_pages):
            url = f"{base_url}?text={query}&lr={lr}&p={page}"
            print(f"\n--- –¢–µ–º–∞: '{query}' | –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}: {url}")

            driver.get(url)

            # –ñ–¥—ë–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'li.serp-item_card'))
                )

                wait_for_captcha(driver)

                html = driver.page_source
                new_links = parse_yandex_results(html, query.replace(" ", "_"))
                print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(new_links)}")

                collected_data.extend(new_links)

            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page + 1}: {e}")

            random_pause(5, 8)

    finally:
        driver.quit()

    #–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±—â–∏–π —Ñ–∞–π–ª
    df_new = pd.DataFrame(collected_data)

    if os.path.exists(OUTPUT_FILE):
        df_old = pd.read_csv(OUTPUT_FILE)
        old_urls = set(df_old['url'].values)
        df_new = df_new[~df_new['url'].isin(old_urls)]
        print(f"–ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º {len(collected_data) - len(df_new)} —Å—Å—ã–ª–æ–∫")
    else:
        print("–°–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π —Ñ–∞–π–ª")

    if not df_new.empty:
        df_new.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig", mode='a', header=not os.path.exists(OUTPUT_FILE))
        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫: {len(df_new)}")
    else:
        print("–ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–µ—Ç.")

#–ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º
if __name__ == "__main__":
    seen_urls = set()

    for topic in SEARCH_TOPICS:
        print(f"\n–ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –ø–æ —Ç–µ–º–µ: {topic}")
        collect_links(topic, num_pages=NUM_PAGES, lr=lr)